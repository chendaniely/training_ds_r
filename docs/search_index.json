[
["index.html", "Training Manual Welcome ", " Training Manual Daniel Chen 2020-02-18 Welcome "],
["lesson-materials.html", "Lesson Materials", " Lesson Materials The main source for the training materials come from Software-Carpentry (Wilson 2016), specifically the Bash, Git (Ahmadia et al. 2016), and SQL lessons. More references about Software-Carpentry and the challenges in scientific computing can be found here: (Wilson, n.d.) (Wilson 2009) (Hannay et al. 2009) (Wilson 2008) (Wilson 2006) (Wilson 2005) Software-Carpenty Data-Carpentry The Carpentries DataCamp R for Data Science By: Garrett Grolemund and Hadley Wickham http://r4ds.had.co.nz/ Figure 0.1: R for Data Science Cover References "],
["infrastructure.html", "Chapter 1 Infrastructure ", " Chapter 1 Infrastructure "],
["project-template.html", "1.1 Project Template", " 1.1 Project Template Why project templates: https://chendaniely.github.io/sdal/2017/05/30/project_templates/ Other resources: https://github.com/ropensci/rrrpkg https://github.com/benmarwick/rrtools A template for research projects structured as R packages: https://github.com/Pakillo/template project/ # the project/code repository | |- data/ # raw and primary data, are not changed once created | | | +- *project_data/ # subfolder that links to an encrypted data storage container | | | note that nothing here is in the git repository | | | it&#39;s just a shortcut (i.e., link) to a different folder | | |- original/ # raw data, will not be altered | | |- working/ # intermediate datasets from src code | | +- final/ # final datasets used for analysis/models/plots | | | +- more_data/ # some projects will need multiple links | |- src/ # any programmatic code | |- analysis1/ # user1 assigned to the project | +- analysis2/ # user2 assigned to the project | |- R/ # functions | |- tests/ # unit tests | |- output # all output and results from workflows and analyses | |- figures/ # graphs, likely designated for manuscript figures | |- pictures/ # diagrams, images, and other non-graph graphics | +- analysis/ # generated reports for (e.g. rmarkdown output) | |- README.md # the top level description of content | |- .gitignore # git ignore file |- project.Rproj # RStudio project | |- DESCRIPTION # Description file to repo into R package, if applicable +- Makefile # Makefile, if applicable "],
["documentation-and-metadata.html", "Chapter 2 Documentation and Metadata", " Chapter 2 Documentation and Metadata If I asked you next week, next month, 3 months, 1 year from now, where a dataset or figure came from. Would you be able to tell me within 10 minutes? You can write all the metadata information on the toplevel README.md file. If you don’t want to deal with merge conflicts, you can "],
["code-documentaion.html", "2.1 Code documentaion", " 2.1 Code documentaion Each R script should perform a single task (if you have a 1000+ long file, you’re probably doing something wrong). For example: 01-data_ingestion.R 02-data_clean.R 03-data_visualize.R 04-data_output.R Each script should have a short description on the top that explains what it is doing. If the script is part of a pipline. It should also document where the input data/script is coming from. If you wrote a function in the script make sure it has a docstring that explains what it does and what the inputs and outputs are. For example: #&#39; squares a given value #&#39; #&#39; x: a value to square #&#39; return: a numeric value my_square &lt;- function(x): return(x ** 2) Make sure the libraries that are loaded are towards the top of the script. There should not be a library call in the middle of your script. This helps figuring out what packages are needed. Functions you’ve written should be towards the top (and documented) of the script as well. Usually it is under the library loading. This helps separate your functions from the code. If the script does not take too long to run, you should test your script by restarting an R session. And running the script from top to bottom. To reset your R session, you can: Click the red button on the top right corner of rstudio In RStudio: command/ctrl + shift + F10 In RStudio type: .rs.restartR() in the terminal This will make sure you have a totally clean enviornment when you are testing and running your script. It’s even ‘better’ than using ls(list = ls()) since it will also detach loaded packages. 2.1.1 lintr Using a linter helps find potential errors in your code. For example, variables that you don’t use. It also checks to conform code to a common code style, all of which help make code easier to read for other people/collaboratiors. To lint your script, you can run lintr::lint(&#39;my_r_script.R&#39;) RStudio will open a static code analysis “Markers” tab "],
["metadata.html", "2.2 Metadata", " 2.2 Metadata Now that your scripts are documented, you need to start documenting the inputs and outputs of your file. Ideally all the non-original data can be recreated from your R code, and those instructions are placed in a master script file like a Makefile or bash script. But we’ll keep it simple for now, and just document the process. 2.2.0.1 Datasets For each input dataset, a comment about what is in the dataset, where it came from, and what you used it for should all be listed. You can use a format like this: - my_awesome_dataset.csv - ./data/folder/original/awesome/my_awesome_dataset.csv - contains data about how awesome the various datasets are - used to calculate the &#39;awesome&#39; metric We use the original/working/final folders in our data folder. The working and Final datasets should list what script it comes from. - web_scraped_data.RData - data scraped from the web - comes from ./src/dan/web/scraping.R 2.2.0.2 Reports/Posters/etc Create a doc folder on the top level. What ever your ‘final’ version of a poster or report should be here and checked in. 2.2.0.3 Figures and Tables Each figure and table used in the poster should list the script it comes from. The exact method on how to regenerate that figure should be throughly listed - ./output/poster_fig_1.png - scatter plot for the amazing data - generated by: `./src/dan/amazing/plot_code.R` "],
["readme-files.html", "2.3 README files", " 2.3 README files The top level README file should contain all the information about what the project is and how to get started. It should link to (or contain) the metadata information from the previous example. Remember this is the first thing people will see when they open a repository, the more information here about where things are, the better. 2.3.1 Example File # Datasets - my_awesome_dataset.csv - ./data/folder/original/awesome/my_awesome_dataset.csv - contains data about how awesome the various datasets are - used to calculate the &#39;awesome&#39; metric - web_scraped_data.RData - data scraped from the web - comes from ./src/dan/web/scraping.R # Figures - ./output/poster_fig_1.png - scatter plot for the amazing data - generated by: `./src/dan/amazing/plot_code.R` - ./output/poster_fig_2.pdf - violin plotfor the amazing data - generated by: `./src/dan/amazing/plot_code_42.R` "],
["version-control.html", "Chapter 3 Version Control", " Chapter 3 Version Control Software-Carpentry Git Lesson DataCamp Courses: Introduction to Git for Data Science Working with the RStudio IDE (Part 2) – Chapter 2: Version Control Quick References: Software-Carpentry Reference Git Cheat Sheet (by Github) Jenny Bryan’s “Happy Git and GitHub for the useR” Git interaction from NDP Software Learn Git Branching Git flight rules for when you mess up "],
["git.html", "3.1 Git", " 3.1 Git Git and the “final” version problem If these comics bring back haunting memories, then version control is for you! Technically, renaming copies of files is a form of version control. It allows you to go back to a specific state of a file. As the two comics point out, this usually ends up in a cacophony of files with similar names. What about files and programs that know how to track changes already. I’m mainly thinking about Word documents. "],
["git-setup.html", "3.2 Git setup", " 3.2 Git setup In a terminal: git config --global user.name &quot;Vlad Dracula&quot; git config --global user.email &quot;vlad@tran.sylvan.ia&quot; git config --global color.ui auto git config --global core.editor &quot;nano -w&quot; Optional setup (aliases) git config --global alias.l &#39;log --oneline --graph --decorate --all&#39; git config --global alias.last &#39;log -1 HEAD&#39; git config --global alias.co checkout git config --global alias.br branch git config --global alias.ci commit git config --global alias.st status "],
["git-on-your-own.html", "3.3 Git on your own", " 3.3 Git on your own Figure 3.1: Diagram of Git commands and how they relate to one another. git init: turn the current folder into a git repository git status: let’s you know what is going on run this all the time! git add: put file(s) into your “staging area” git commit -m 'MY COMMIT MESSAGE': commits files in the “staging area” with the given message git diff &lt;file name&gt;: compares saved changes to a file to the last commited version of the file git diff --staged &lt;file name&gt;: compares a staged file to the last commited version of the file git log and git log --oneline: looks at your git history git log --oneline --graph --decorate --all: gives detailed log information about where you are How not to write commit messages: how #not to write #git #commit messages -.-'' pic.twitter.com/5TdiZ1yi5S — Dⓐniel Chen ((???)) April 16, 2015 "],
["working-with-remotes.html", "3.4 Working with remotes", " 3.4 Working with remotes git clone &lt;repo url&gt;: downloads code from a code repository into your current directory git remote -v: lists all the remotes and their short names (e.g., origin, upstream) git remote add &lt;name&gt; &lt;url&gt;: adds the &lt;url&gt; to your remotes and gives it the short name &lt;name&gt; (e.g., git remote add origin &lt;URL&gt;) git remote rm &lt;name&gt;: removes a remote by its shortname git push &lt;where&gt; &lt;what&gt;: pushes code on the &lt;what&gt; branch to the &lt;where&gt; remote (e.g., git push origin master) git pull &lt;where&gt; &lt;what&gt;: pulls does down from the &lt;what&gt; branch from the &lt;where&gt; remote (e.g., git pull origin master) "],
["git-with-branches.html", "3.5 Git with branches", " 3.5 Git with branches Figure 0.1: Review of Git Figure 3.2: What branching looks like in the Git world git branch &lt;new branch name&gt;: creates a new branch called &lt;new branch name&gt; git checkout &lt;new branch name&gt;: goes to the branch, &lt;new branch name&gt; git checkout -b &lt;new branch name&gt;: creates and checksout a branch in a single step git log --oneline --graph --decorate --all: shows you the log in relation to all other branches "],
["collaborating-with-git.html", "3.6 Collaborating with Git", " 3.6 Collaborating with Git This is like working with branches, but instead of you working on a branch, it’s someone else Figure 3.3: The ‘forking’ model of Git workflows Figure 3.4: Git with branches "],
["protecting-branches.html", "3.7 Protecting branches", " 3.7 Protecting branches https://docs.gitlab.com/ee/user/project/protected_branches.html In a repository go to settings &gt; repository &gt; protected branches set “allowed to merge”: masters “allowed to push”: no one "],
["help-faq.html", "3.8 Help! (FAQ)", " 3.8 Help! (FAQ) Link to more “flight rules”, for when you really mess up. 3.8.1 General workflow Make sure your master branch is up to date Go to the master branch: git checkout master Update your master branch: git pull origin master Create a new branch (give it a useful name): git checkout -b my_awesome_task Give your branch a sensible name about what you are working on Don’t just give it your name or pid, nobody knows what you are doing Go code and write commits! git add &lt;my file&gt; git commit -m 'look at all this cool stuff' Push your branch git push origin my_awesome_task Issue a pull request Have someone (maintainers) review your code Does it follow coding style guides? Is the code “good” No datasets are checked in No loops when an apply or map function would suffice There are functions for repetitive code Are you checking your work? Are the assumptions you are making about data tested in code? If you are visually checking/inspecting your data to check your code, is there code written for your visual check? etc (Maintainers) merge the pull request You can also checkoff a box that will also delete the branch on the remote Or delete the branch manually under the branches view Go back to master: git checkout master Pull down your merged code: git pull origin master Delete your branch: git branch -d my_awesome_task (note it is a lower case d) Clean up your branches: git fetch --prune 3.8.2 Git push rejected (master) When you run git status, does it say you’re on master? Please see below: “Accidently did work on master”. 3.8.3 Accidently did work on master: Create a branch where you are now: git branch BRANCH_NAME Find the commit hash of where master is supposed to be git log --oneline --graph --decorate --all Reset master to where you were: git reset --hard COMMIT_HASH_FOR_MASTER make sure you do this on the master branch Go to your branch: git checkout BRANCH_NAME Push your branch: git push origin BRANCH_NAME Create and merge the pull/merge request 3.8.4 Get changes from master on your branch Scenario: You are working on your branch, and the master branch changes (e.g., someone else gets their branch merged into master). The changes in master are also changes you need (e.g., the update to master is a function that you want to use), but you are still working on your branch and not ready to create a pull/merge request and/or merge your changes yet. Go to your master branch: git checkout master Get the new updates from master: git pull origin master Go back to your branch: git checkout my_branch Rebase your branch against master: git rebase master You may or may not need to solve merge conflicts. Force push your branch: git push -f origin my_branch 3.8.5 Remote server (e.g., GitLab, GitHub, Bitbucket, etc) shows merge conflict When you attempt to merge a pull/merge request and tells you that the branch cannot be merged becuase of a merge conflict, you need to follow the same steps from “Get changes from master on your branch”. During the rebasing step, you will be fixing the merge conflict(s). 3.8.6 Remove data/files from history 3.8.6.1 BFG repo cleaner https://rtyley.github.io/bfg-repo-cleaner/ 3.8.6.2 Git filter branch git filter-branch --force --index-filter \\ &#39;git rm --cached --ignore-unmatch &lt;PATH_TO_FILE&gt;&#39; \\ --prune-empty --tag-name-filter cat -- --all as a single line: git filter-branch --force --index-filter &#39;git rm --cached --ignore-unmatch &lt;PATH_TO_FILE&gt;&#39; --prune-empty --tag-name-filter cat -- --all "],
["rstudio.html", "Chapter 4 RStudio ", " Chapter 4 RStudio "],
["restarting-rstudio-session.html", "4.1 Restarting RStudio Session", " 4.1 Restarting RStudio Session 4.1.1 Within RStudio Session &gt; Restart R (Ctrl + Shift + F10) 4.1.2 From the terminal When RStudio locks up and you need to restart it. Note you will need to be on the VPN if you are not on the VT wireless. Open a terminal Look for your rstudio process by running: ps aux | grep rstudio &gt; ps aux | grep chend | grep rstudio chend 163630 0.3 0.0 743120 100936 ? Sl 09:51 0:04 /usr/lib/rstudio-server/bin/rsession -u chend --launcher-token 59FDC2A5 chend 182064 0.0 0.0 112708 968 pts/0 S+ 10:14 0:00 grep --color=auto rstudio Note which process rstudio is running on by looking at the 2nd column in the output (next to your pid), in this example the process number is 163630 Stop the process: kill -9 &lt;Process number here&gt; "],
["accessing-folders.html", "4.2 Accessing folders", " 4.2 Accessing folders 4.2.1 Outside of Home Sometimes you might want to navigate outside your home directory. To do this first click the 3 dots in the files panel to manually input a directory. Figure 3.1: Go to a specific directory Then you can manually navigate into a different path. Figure 0.1: Manually input directory This example specifically shows navigating to the /home/sdal/projects directory. However, you might want to check your project’s data folder, a link to the relevant data folder should already be in there. "],
["functions.html", "Chapter 5 Functions ", " Chapter 5 Functions "],
["writing-functions.html", "5.1 Writing Functions", " 5.1 Writing Functions 5.1.1 Fahrenheit to Kelvin \\(k = ((f - 32) * (5 / 9)) + 273.15\\) ((32 - 32) * (5 / 9)) + 273.15 ## [1] 273.15 ((212 - 32) * (5 / 9)) + 273.15 ## [1] 373.15 ((-42 - 32) * (5 / 9)) + 273.15 ## [1] 232.0389 f_k &lt;- function(f_temp) { ((f_temp - 32) * (5 / 9)) + 273.15 } f_k(32) ## [1] 273.15 f_k(212) ## [1] 373.15 f_k(-42) ## [1] 232.0389 5.1.2 Kelvin to Celsius k_c &lt;- function(temp_k) { temp_c &lt;- temp_k - 273.15 return(temp_c) } k_c(0) ## [1] -273.15 5.1.3 Fahrenheit to Celsius f_c &lt;- function(temp_f) { temp_k &lt;- f_k(temp_f) temp_c &lt;- k_c(temp_k) return(temp_c) } f_c(32) ## [1] 0 f_c(212) ## [1] 100 "],
["testing-functions.html", "5.2 Testing Functions", " 5.2 Testing Functions library(testthat) testthat::expect_equal(f_c(32), 0) testthat::expect_equal(f_c(212), 100) "],
["exercise.html", "5.3 Exercise", " 5.3 Exercise What happens if you use NA, Inf, -Inf in your function? What are some better names to give the functions we wrote? How would you name these functions in a package? "],
["checking-values.html", "5.4 Checking values", " 5.4 Checking values Calculating weighted means mean_wt &lt;- function(x, w) { sum(x * w) / sum(w) } mean_wt(1:6, 1:6) ## [1] 4.333333 If you expect the lengths to be the same, then you should test for it in the function mean_wt(1:6, 1:3) ## [1] 7.666667 mean_wt &lt;- function(x, w) { if (length(x) != length(w)) { stop(&quot;`x` and `w` should be the same length&quot;) } sum(x * w) / sum(w) } mean_wt(1:6, 1:3) ## Error in mean_wt(1:6, 1:3): `x` and `w` should be the same length "],
["dot-dot-dot.html", "5.5 dot-dot-dot …", " 5.5 dot-dot-dot … Use it to pass on arguments to another function inside. But you can also use it to force named arguments in your function. sum_3 &lt;- function(x, y, z) { return(x + y + z) } sum_3(1, 2, 3) ## [1] 6 sum_3 &lt;- function(x, y, ..., z) { return(x + y + z) } sum_3(1, 2, z = 3) ## [1] 6 sum_3(1, 2, z = 3) ## [1] 6 "],
["conditionals.html", "Chapter 6 Conditionals ", " Chapter 6 Conditionals "],
["if-statements.html", "6.1 if statements", " 6.1 if statements # make a modification to this function k_c &lt;- function(temp_k) { if (temp_k &lt; 0) { warning(&#39;you passed in a negative Kelvin number&#39;) # stop() return(NA) } temp_c &lt;- temp_k - 273.15 return(temp_c) } k_c(-9) ## Warning in k_c(-9): you passed in a negative Kelvin number ## [1] NA Our current function does not deal with missing numbers k_c(NA) Error in if (temp_k &lt; 0) { : missing value where TRUE/FALSE needed k_c(0) ## [1] -273.15 "],
["if-else-statements.html", "6.2 If else statements", " 6.2 If else statements k_c &lt;- function(temp_k) { if (temp_k &lt; 0) { warning(&#39;you passed in a negative Kelvin number&#39;) # stop() return(NA) } else { temp_c &lt;- temp_k - 273.15 return(temp_c) } } k_c(-9) ## Warning in k_c(-9): you passed in a negative Kelvin number ## [1] NA Our current function does not deal with missing numbers k_c(NA) k_c(0) ## [1] -273.15 "],
["dealing-with-na.html", "6.3 Dealing with NA", " 6.3 Dealing with NA Re-write our function to work with missing values. Note you need to make the NA check first. k_c &lt;- function(temp_k) { if (is.na(temp_k)) { return(NA) } else if (temp_k &lt; 0) { warning(&#39;you passed in a negative Kelvin number&#39;) # stop() return(NA) } else { temp_c &lt;- temp_k - 273.15 return(temp_c) } } k_c(-9) ## Warning in k_c(-9): you passed in a negative Kelvin number ## [1] NA k_c(NA) ## [1] NA k_c(0) ## [1] -273.15 if (c(TRUE, FALSE)) {} ## Warning in if (c(TRUE, FALSE)) {: the condition has length &gt; 1 and only the ## first element will be used ## NULL if (NA) {} ## Error in if (NA) {: missing value where TRUE/FALSE needed use &amp;&amp; and || to short-circuit the boolean comparisons. This will also guarantee a value of length 1L. == is also vectorized, should use identical() or all.equal(). identical is very strict. Doesn’t corece types. identical(0L, 0) ## [1] FALSE all.equal has ability to set tolerances. all.equal: compare R objects x and y testing ‘near equality’. If they are different, comparison is still made to some extent, and a report of the differences is returned. Do not use all.equal directly in if expressions—either use isTRUE(all.equal(….)) or identical if appropriate. all.equal(0L, 0) ## [1] TRUE if (isTRUE(all.equal(0L, 0))) {print(&quot;Hello&quot;)} ## [1] &quot;Hello&quot; "],
["fizzbuzz.html", "6.4 Fizzbuzz", " 6.4 Fizzbuzz fizzbuzz &lt;- function(x) { # these two lines check that x is a valid input stopifnot(length(x) == 1) stopifnot(is.numeric(x)) if (!(x %% 3) &amp;&amp; !(x %% 5)) { &quot;fizzbuzz&quot; } else if (!(x %% 3)) { &quot;fizz&quot; } else if (!(x %% 5)) { &quot;buzz&quot; } else { # ensure that the function returns a character vector as.character(x) } } fizzbuzz(6) ## [1] &quot;fizz&quot; Check modulo 3 only once fizzbuzz2 &lt;- function(x) { # these two lines check that x is a valid input stopifnot(length(x) == 1) stopifnot(is.numeric(x)) if (!(x %% 3)) { if (!(x %% 5)) { &quot;fizzbuzz&quot; } else { &quot;fizz&quot; } } else if (!(x %% 5)) { &quot;buzz&quot; } else { # ensure that the function returns a character vector as.character(x) } } fizzbuzz(6) ## [1] &quot;fizz&quot; 6.4.1 Vectorized conditionals library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union fizzbuzz_vec &lt;- function(x) { dplyr::case_when( !(x %% 3) &amp; !(x %% 5) ~ &quot;fizzbuzz&quot;, !(x %% 3) ~ &quot;fizz&quot;, !(x %% 5) ~ &quot;buzz&quot;, TRUE ~ as.character(x) ) } fizzbuzz(1:10) ## Error in fizzbuzz(1:10): length(x) == 1 is not TRUE fizzbuzz_vec(1:10) ## [1] &quot;1&quot; &quot;2&quot; &quot;fizz&quot; &quot;4&quot; &quot;buzz&quot; &quot;fizz&quot; &quot;7&quot; &quot;8&quot; &quot;fizz&quot; &quot;buzz&quot; 6.4.2 Multiple conditions if (this) { # do that } else if (that) { # do something else } else { # } 6.4.2.1 switch calc_op &lt;- function(x, y, op) { switch(op, plus = x + y, minus = x - y, times = x * y, divide = x / y, stop(&quot;Unknown op!&quot;) ) } calc_op(10, 20, &quot;times&quot;) ## [1] 200 calc_op(10, 20, &quot;divide&quot;) ## [1] 0.5 6.4.2.2 cut describe_temp &lt;- function(temp) { if (temp &lt;= 0) { &quot;freezing&quot; } else if (temp &lt;= 10) { &quot;cold&quot; } else if (temp &lt;= 20) { &quot;cool&quot; } else if (temp &lt;= 30) { &quot;warm&quot; } else { &quot;hot&quot; } } describe_temp(16) ## [1] &quot;cool&quot; Current function can’t handle vectors describe_temp(c(16, 61)) ## Warning in if (temp &lt;= 0) {: the condition has length &gt; 1 and only the first ## element will be used ## Warning in if (temp &lt;= 10) {: the condition has length &gt; 1 and only the first ## element will be used ## Warning in if (temp &lt;= 20) {: the condition has length &gt; 1 and only the first ## element will be used ## [1] &quot;cool&quot; How cut works: values &lt;- -10:10 values ## [1] -10 -9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 ## [20] 9 10 cut(values, c(-Inf, -5, -1, 1, 7, Inf)) ## [1] (-Inf,-5] (-Inf,-5] (-Inf,-5] (-Inf,-5] (-Inf,-5] (-Inf,-5] (-5,-1] ## [8] (-5,-1] (-5,-1] (-5,-1] (-1,1] (-1,1] (1,7] (1,7] ## [15] (1,7] (1,7] (1,7] (1,7] (7, Inf] (7, Inf] (7, Inf] ## Levels: (-Inf,-5] (-5,-1] (-1,1] (1,7] (7, Inf] cut(values, c(-Inf, -5, -1, 1, 7, Inf), labels = LETTERS[1:5], right = TRUE) ## [1] A A A A A A B B B B C C D D D D D D E E E ## Levels: A B C D E cut(values, c(-Inf, -5, -1, 1, 7, Inf), labels = LETTERS[1:5], right = FALSE) ## [1] A A A A A B B B B C C D D D D D D E E E E ## Levels: A B C D E "],
["exercise-1.html", "6.5 Exercise", " 6.5 Exercise Rewrite the function using cut describe_temp &lt;- function(temp) { if (temp &lt;= 0) { &quot;freezing&quot; } else if (temp &lt;= 10) { &quot;cold&quot; } else if (temp &lt;= 20) { &quot;cool&quot; } else if (temp &lt;= 30) { &quot;warm&quot; } else { &quot;hot&quot; } } How do you indicate &lt; and &lt;=? "],
["iteration.html", "Chapter 7 Iteration ", " Chapter 7 Iteration "],
["broadcasting.html", "7.1 Broadcasting", " 7.1 Broadcasting f_values &lt;- c(0, 32, 212, -40) f_values * 10 ## [1] 0 320 2120 -400 f_values * c(10, 100) ## [1] 0 3200 2120 -4000 "],
["for-loops.html", "7.2 For loops", " 7.2 For loops Temp conversion functions f_k &lt;- function(f_temp) { ((f_temp - 32) * (5 / 9)) + 273.15 } k_c &lt;- function(temp_k) { if (is.na(temp_k)) { return(NA) } else if (temp_k &lt; 0) { warning(&#39;you passed in a negative Kelvin number&#39;) # stop() return(NA) } else { temp_c &lt;- temp_k - 273.15 return(temp_c) } } f_c &lt;- function(temp_f) { temp_k &lt;- f_k(temp_f) temp_c &lt;- k_c(temp_k) return(temp_c) } for (pizza in f_values) { print(pizza) converted &lt;- f_c(pizza) print(converted) } ## [1] 0 ## [1] -17.77778 ## [1] 32 ## [1] 0 ## [1] 212 ## [1] 100 ## [1] -40 ## [1] -40 # 1:length(f_values) # seq_along(f_values) for (i in seq_along(f_values)) { print(i) val &lt;- f_values[i] print(val) converted &lt;- f_c(val) print(converted) } ## [1] 1 ## [1] 0 ## [1] -17.77778 ## [1] 2 ## [1] 32 ## [1] 0 ## [1] 3 ## [1] 212 ## [1] 100 ## [1] 4 ## [1] -40 ## [1] -40 7.2.1 Pre allocating in a loop # prepopulate an empty vector converted_values &lt;- vector(&quot;double&quot;, length(f_values)) for (to_be_converted_position in seq_along(f_values)) { converted &lt;- f_c(to_be_converted_position) converted_values[to_be_converted_position] &lt;- converted } converted_values ## [1] -17.22222 -16.66667 -16.11111 -15.55556 "],
["purrr-map.html", "7.3 purrr (map)", " 7.3 purrr (map) library(purrr) map(f_values, f_c) ## [[1]] ## [1] -17.77778 ## ## [[2]] ## [1] 0 ## ## [[3]] ## [1] 100 ## ## [[4]] ## [1] -40 map_dbl(f_values, f_c) ## [1] -17.77778 0.00000 100.00000 -40.00000 mtcars ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 map(mtcars, class) ## $mpg ## [1] &quot;numeric&quot; ## ## $cyl ## [1] &quot;numeric&quot; ## ## $disp ## [1] &quot;numeric&quot; ## ## $hp ## [1] &quot;numeric&quot; ## ## $drat ## [1] &quot;numeric&quot; ## ## $wt ## [1] &quot;numeric&quot; ## ## $qsec ## [1] &quot;numeric&quot; ## ## $vs ## [1] &quot;numeric&quot; ## ## $am ## [1] &quot;numeric&quot; ## ## $gear ## [1] &quot;numeric&quot; ## ## $carb ## [1] &quot;numeric&quot; map_chr(mtcars, class) ## mpg cyl disp hp drat wt qsec vs ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ## am gear carb ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; map_dbl(mtcars, mean) ## mpg cyl disp hp drat wt qsec ## 20.090625 6.187500 230.721875 146.687500 3.596563 3.217250 17.848750 ## vs am gear carb ## 0.437500 0.406250 3.687500 2.812500 map(mtcars, summary) ## $mpg ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 10.40 15.43 19.20 20.09 22.80 33.90 ## ## $cyl ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.000 4.000 6.000 6.188 8.000 8.000 ## ## $disp ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 71.1 120.8 196.3 230.7 326.0 472.0 ## ## $hp ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 52.0 96.5 123.0 146.7 180.0 335.0 ## ## $drat ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.760 3.080 3.695 3.597 3.920 4.930 ## ## $wt ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.513 2.581 3.325 3.217 3.610 5.424 ## ## $qsec ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 14.50 16.89 17.71 17.85 18.90 22.90 ## ## $vs ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000 0.0000 0.0000 0.4375 1.0000 1.0000 ## ## $am ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000 0.0000 0.0000 0.4062 1.0000 1.0000 ## ## $gear ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3.000 3.000 4.000 3.688 4.000 5.000 ## ## $carb ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 2.000 2.000 2.812 4.000 8.000 7.3.1 Exercise Compute the mean of every column in mtcars. Determine the type of each column in nycflights13::flights. Compute the number of unique values in each column of iris. (Hint: you may want to write a function) Generate 10 random normals from distributions with means of -10, 0, 10, and 100. 7.3.1.1 Solutions # 1. Compute the mean of every column in mtcars purrr::map_dbl(mtcars, mean) ## mpg cyl disp hp drat wt qsec ## 20.090625 6.187500 230.721875 146.687500 3.596563 3.217250 17.848750 ## vs am gear carb ## 0.437500 0.406250 3.687500 2.812500 # 2. Determine the type of each column in nycflights13::flights. purrr::map_chr(nycflights13::flights, class) ## Error: Result 19 must be a single string, not a character vector of length 2 purrr::map(nycflights13::flights, class) ## $year ## [1] &quot;integer&quot; ## ## $month ## [1] &quot;integer&quot; ## ## $day ## [1] &quot;integer&quot; ## ## $dep_time ## [1] &quot;integer&quot; ## ## $sched_dep_time ## [1] &quot;integer&quot; ## ## $dep_delay ## [1] &quot;numeric&quot; ## ## $arr_time ## [1] &quot;integer&quot; ## ## $sched_arr_time ## [1] &quot;integer&quot; ## ## $arr_delay ## [1] &quot;numeric&quot; ## ## $carrier ## [1] &quot;character&quot; ## ## $flight ## [1] &quot;integer&quot; ## ## $tailnum ## [1] &quot;character&quot; ## ## $origin ## [1] &quot;character&quot; ## ## $dest ## [1] &quot;character&quot; ## ## $air_time ## [1] &quot;numeric&quot; ## ## $distance ## [1] &quot;numeric&quot; ## ## $hour ## [1] &quot;numeric&quot; ## ## $minute ## [1] &quot;numeric&quot; ## ## $time_hour ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; # Compute the number of unique values in each column of iris. count_unique &lt;- function(x) { return(length(unique(x))) } purrr::map_int(iris, count_unique) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 35 23 43 22 3 purrr::map_int(iris, function(x) length(unique(x))) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 35 23 43 22 3 purrr::map_int(iris, ~ length(unique(.))) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 35 23 43 22 3 # Generate 10 random normals from distributions with means of -10, 0, 10, and 100. purrr::map(c(-10, 0, 10, 100), ~ rnorm(n = 10, mean = .)) ## [[1]] ## [1] -8.629042 -10.564698 -9.636872 -9.367137 -9.595732 -10.106125 ## [7] -8.488478 -10.094659 -7.981576 -10.062714 ## ## [[2]] ## [1] 1.3048697 2.2866454 -1.3888607 -0.2787888 -0.1333213 0.6359504 ## [7] -0.2842529 -2.6564554 -2.4404669 1.3201133 ## ## [[3]] ## [1] 9.693361 8.218692 9.828083 11.214675 11.895193 9.569531 9.742731 ## [8] 8.236837 10.460097 9.360005 ## ## [[4]] ## [1] 100.45545 100.70484 101.03510 99.39107 100.50496 98.28299 99.21554 ## [8] 99.14909 97.58579 100.03612 "],
["fitting-models.html", "7.4 Fitting models", " 7.4 Fitting models models &lt;- mtcars %&gt;% split(.$cyl) %&gt;% map(function(df) lm(mpg ~ wt, data = df)) models %&gt;% map(summary) %&gt;% map_dbl(~ .$r.squared) ## 4 6 8 ## 0.5086326 0.4645102 0.4229655 models %&gt;% map_df(broom::tidy) ## # A tibble: 6 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 39.6 4.35 9.10 0.00000777 ## 2 wt -5.65 1.85 -3.05 0.0137 ## 3 (Intercept) 28.4 4.18 6.79 0.00105 ## 4 wt -2.78 1.33 -2.08 0.0918 ## 5 (Intercept) 23.9 3.01 7.94 0.00000405 ## 6 wt -2.19 0.739 -2.97 0.0118 "],
["apply-in-base-r.html", "7.5 Apply (in base R)", " 7.5 Apply (in base R) apply family of functions 7.5.1 lapply lapply(f_values, f_c) ## [[1]] ## [1] -17.77778 ## ## [[2]] ## [1] 0 ## ## [[3]] ## [1] 100 ## ## [[4]] ## [1] -40 7.5.2 sapply sapply(f_values, f_c) ## [1] -17.77778 0.00000 100.00000 -40.00000 7.5.3 vapply vapply(f_values, f_c, numeric(1)) ## [1] -17.77778 0.00000 100.00000 -40.00000 7.5.4 mapply v1 &lt;- c(1, 2, 3, 4) v2 &lt;- c(100, 200, 300, 400) my_mean &lt;- function(x, y){ return((x + y) / 2) } # sapply(v1, v2, my_mean) mapply(my_mean, v1, v2) ## [1] 50.5 101.0 151.5 202.0 # this is the same as purrr::map2 7.5.5 apply (2-dimensions) apply(mtcars, MARGIN = 1, mean) ## Mazda RX4 Mazda RX4 Wag Datsun 710 Hornet 4 Drive ## 29.90727 29.98136 23.59818 38.73955 ## Hornet Sportabout Valiant Duster 360 Merc 240D ## 53.66455 35.04909 59.72000 24.63455 ## Merc 230 Merc 280 Merc 280C Merc 450SE ## 27.23364 31.86000 31.78727 46.43091 ## Merc 450SL Merc 450SLC Cadillac Fleetwood Lincoln Continental ## 46.50000 46.35000 66.23273 66.05855 ## Chrysler Imperial Fiat 128 Honda Civic Toyota Corolla ## 65.97227 19.44091 17.74227 18.81409 ## Toyota Corona Dodge Challenger AMC Javelin Camaro Z28 ## 24.88864 47.24091 46.00773 58.75273 ## Pontiac Firebird Fiat X1-9 Porsche 914-2 Lotus Europa ## 57.37955 18.92864 24.77909 24.88027 ## Ford Pantera L Ferrari Dino Maserati Bora Volvo 142E ## 60.97182 34.50818 63.15545 26.26273 apply(mtcars, MARGIN = 2, mean) ## mpg cyl disp hp drat wt qsec ## 20.090625 6.187500 230.721875 146.687500 3.596563 3.217250 17.848750 ## vs am gear carb ## 0.437500 0.406250 3.687500 2.812500 "],
["safely-dealing-with-failure.html", "7.6 Safely dealing with failure", " 7.6 Safely dealing with failure safely is a function that takes a function, and returns a modified version of that function. Like how decorators work in Python. safe_log &lt;- safely(log) safe_log(10) ## $result ## [1] 2.302585 ## ## $error ## NULL safe_log(&quot;a&quot;) ## $result ## NULL ## ## $error ## &lt;simpleError in .Primitive(&quot;log&quot;)(x, base): non-numeric argument to mathematical function&gt; x &lt;- list(1, 10, &quot;a&quot;) y &lt;- x %&gt;% map(safely(log)) str(y) ## List of 3 ## $ :List of 2 ## ..$ result: num 0 ## ..$ error : NULL ## $ :List of 2 ## ..$ result: num 2.3 ## ..$ error : NULL ## $ :List of 2 ## ..$ result: NULL ## ..$ error :List of 2 ## .. ..$ message: chr &quot;non-numeric argument to mathematical function&quot; ## .. ..$ call : language .Primitive(&quot;log&quot;)(x, base) ## .. ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;simpleError&quot; &quot;error&quot; &quot;condition&quot; You can specify the error value safe_log &lt;- safely(log, otherwise = NA) x &lt;- list(1, 10, &quot;a&quot;) y &lt;- x %&gt;% map(safe_log) Extract values out manually y %&gt;% purrr::map_dbl(magrittr::extract(1)) ## [1] 0.000000 2.302585 NA Use the transpose function y &lt;- y %&gt;% purrr::transpose() str(y) ## List of 2 ## $ result:List of 3 ## ..$ : num 0 ## ..$ : num 2.3 ## ..$ : logi NA ## $ error :List of 3 ## ..$ : NULL ## ..$ : NULL ## ..$ :List of 2 ## .. ..$ message: chr &quot;non-numeric argument to mathematical function&quot; ## .. ..$ call : language .Primitive(&quot;log&quot;)(x, base) ## .. ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;simpleError&quot; &quot;error&quot; &quot;condition&quot; y$result %&gt;% purrr::flatten_dbl() ## [1] 0.000000 2.302585 NA "],
["possibly-and-quietly-succeeds.html", "7.7 Possibly and quietly succeeds", " 7.7 Possibly and quietly succeeds Possibly always succeeds by giving it a default value. Safely defaults to NULL, possibly requres the otherwise parameter x &lt;- list(1, 10, &quot;a&quot;) x %&gt;% map_dbl(possibly(log, NA_real_)) ## [1] 0.000000 2.302585 NA quietly captures output, messages, and warnings, instead of errors x &lt;- list(1, -1) x %&gt;% map(quietly(log)) %&gt;% str() ## List of 2 ## $ :List of 4 ## ..$ result : num 0 ## ..$ output : chr &quot;&quot; ## ..$ warnings: chr(0) ## ..$ messages: chr(0) ## $ :List of 4 ## ..$ result : num NaN ## ..$ output : chr &quot;&quot; ## ..$ warnings: chr &quot;NaNs produced&quot; ## ..$ messages: chr(0) x %&gt;% map(quietly(log)) %&gt;% purrr::transpose() %&gt;% str() ## List of 4 ## $ result :List of 2 ## ..$ : num 0 ## ..$ : num NaN ## $ output :List of 2 ## ..$ : chr &quot;&quot; ## ..$ : chr &quot;&quot; ## $ warnings:List of 2 ## ..$ : chr(0) ## ..$ : chr &quot;NaNs produced&quot; ## $ messages:List of 2 ## ..$ : chr(0) ## ..$ : chr(0) "],
["mapping-over-different-arguments.html", "7.8 Mapping over different arguments", " 7.8 Mapping over different arguments map2 and pmap varying just a single value mu &lt;- list(5, 10, -3) mu %&gt;% map(rnorm, n = 5) %&gt;% str() ## List of 3 ## $ : num [1:5] 5.21 4.64 5.76 4.27 3.63 ## $ : num [1:5] 10.43 9.19 11.44 9.57 10.66 ## $ : num [1:5] -2.68 -3.78 -1.42 -2.36 -2.91 varying both mu and sigmna sigma &lt;- list(1, 5, 10) map2(mu, sigma, rnorm, n = 5) %&gt;% str() ## List of 3 ## $ : num [1:5] 5.28 5.68 5.09 2.01 5.28 ## $ : num [1:5] 8.16 10.93 12.91 17 6.36 ## $ : num [1:5] 10.025 0.358 7.385 6.207 4.209 variying across mu, sigma, and n n &lt;- list(1, 3, 5) args &lt;- list(mean = mu, sd = sigma, n = n) pmap(args, rnorm) %&gt;% str() ## List of 3 ## $ : num 3.96 ## $ : num [1:3] 9.55 13.12 5.23 ## $ : num [1:5] -8.43 2.81 4.68 1.64 -11.86 If your data is all the same length it can be stored in a dataframe. Or if you have a dataframe of values you are planning to “apply” over. library(tibble) params &lt;- tribble( ~mean, ~sd, ~n, 5, 1, 1, 10, 5, 3, -3, 10, 5 ) params %&gt;% pmap(rnorm) ## [[1]] ## [1] 3.900219 ## ## [[2]] ## [1] 17.56354 11.28961 10.44220 ## ## [[3]] ## [1] -4.208965 -14.943289 3.119969 -5.171398 -4.827567 Having a dataframe and using list columns, you can change the function an it’s arguments with invoke_map f &lt;- c(&quot;runif&quot;, &quot;rnorm&quot;, &quot;rpois&quot;) param &lt;- list( list(min = -1, max = 1), list(sd = 5), list(lambda = 10) ) invoke_map(f, param, n = 5) %&gt;% str() ## List of 3 ## $ : num [1:5] 0.649 0.185 0.589 0.538 0.836 ## $ : num [1:5] 5.46 -3.23 3.33 4.09 -2.8 ## $ : int [1:5] 12 17 13 13 6 library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union sim &lt;- tribble( ~f, ~params, &quot;runif&quot;, list(min = -1, max = 1), &quot;rnorm&quot;, list(sd = 5), &quot;rpois&quot;, list(lambda = 10) ) sim_invoked &lt;- sim %&gt;% mutate(sim = invoke_map(f, params, n = 10)) sim_invoked ## # A tibble: 3 x 3 ## f params sim ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 runif &lt;named list [2]&gt; &lt;dbl [10]&gt; ## 2 rnorm &lt;named list [1]&gt; &lt;dbl [10]&gt; ## 3 rpois &lt;named list [1]&gt; &lt;int [10]&gt; sim_invoked$sim ## [[1]] ## [1] -0.03082414 -0.49508312 -0.48062004 0.08403188 0.29975168 -0.32716174 ## [7] -0.87810051 -0.09737830 0.67751007 0.14927467 ## ## [[2]] ## [1] -1.881454 6.205581 -4.738676 8.839889 4.586639 -4.473877 ## [7] -3.440830 -4.992420 -10.536207 4.287095 ## ## [[3]] ## [1] 13 9 10 7 6 14 7 15 10 6 "],
["exercise-3.html", "7.9 Exercise", " 7.9 Exercise Write a pipeline that caclculates the mean of each simulation sim_invoked %&gt;% mutate(estimate = map_dbl(sim, mean)) ## # A tibble: 3 x 4 ## f params sim estimate ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;dbl&gt; ## 1 runif &lt;named list [2]&gt; &lt;dbl [10]&gt; -0.110 ## 2 rnorm &lt;named list [1]&gt; &lt;dbl [10]&gt; -0.614 ## 3 rpois &lt;named list [1]&gt; &lt;int [10]&gt; 9.7 "],
["walk.html", "7.10 Walk", " 7.10 Walk the map set of functions return function values, you use walk if you are only interested in the function side effects. E.g., saving out to a file. x &lt;- list(1, &quot;a&quot;, 3) x %&gt;% walk(print) ## [1] 1 ## [1] &quot;a&quot; ## [1] 3 list of plots with file names, you can use pwalk to save library(ggplot2) plots &lt;- mtcars %&gt;% split(.$cyl) %&gt;% map(~ggplot(., aes(mpg, wt)) + geom_point()) paths &lt;- stringr::str_c(names(plots), &quot;.pdf&quot;) # pwalk(list(paths, plots), ggsave, path = tempdir()) "],
["other-predicates.html", "7.11 Other predicates", " 7.11 Other predicates keep, discard only keeps or discards TRUE values iris %&gt;% keep(is.factor) %&gt;% str() ## &#39;data.frame&#39;: 150 obs. of 1 variable: ## $ Species: Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... some and every work like any and all x &lt;- list(1:5, letters, list(10)) x %&gt;% some(is_character) ## [1] TRUE detect and detect_index finds the first element x &lt;- sample(10) x ## [1] 2 8 7 3 5 6 4 1 10 9 x %&gt;% detect(~ . &gt; 5) ## [1] 8 head_while tail_while returns the head or tail while something is true x ## [1] 2 8 7 3 5 6 4 1 10 9 x %&gt;% head_while(~ . &gt; 5) ## integer(0) x %&gt;% tail_while(~ . &gt; 5) ## [1] 10 9 reduce applies a function until there is only one left. Useful for repeadily joinging dataframes together dfs &lt;- list( age = tibble(name = &quot;John&quot;, age = 30), sex = tibble(name = c(&quot;John&quot;, &quot;Mary&quot;), sex = c(&quot;M&quot;, &quot;F&quot;)), trt = tibble(name = &quot;Mary&quot;, treatment = &quot;A&quot;) ) dfs ## $age ## # A tibble: 1 x 2 ## name age ## &lt;chr&gt; &lt;dbl&gt; ## 1 John 30 ## ## $sex ## # A tibble: 2 x 2 ## name sex ## &lt;chr&gt; &lt;chr&gt; ## 1 John M ## 2 Mary F ## ## $trt ## # A tibble: 1 x 2 ## name treatment ## &lt;chr&gt; &lt;chr&gt; ## 1 Mary A dfs %&gt;% reduce(full_join) ## Joining, by = &quot;name&quot; ## Joining, by = &quot;name&quot; ## # A tibble: 2 x 4 ## name age sex treatment ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 John 30 M &lt;NA&gt; ## 2 Mary NA F A "],
["linear-models.html", "Chapter 8 Linear Models ", " Chapter 8 Linear Models "],
["fit-a-linear-model.html", "8.1 Fit a linear model", " 8.1 Fit a linear model 8.1.1 Base R library(AmesHousing) ames &lt;- AmesHousing::make_ames() # Gr Liv Area = Above grade (ground) living area square feet lm_ames &lt;- lm(Sale_Price ~ Gr_Liv_Area, data = ames) lm_ames ## ## Call: ## lm(formula = Sale_Price ~ Gr_Liv_Area, data = ames) ## ## Coefficients: ## (Intercept) Gr_Liv_Area ## 13289.6 111.7 broom::tidy(lm_ames) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 13290. 3270. 4.06 0.0000494 ## 2 Gr_Liv_Area 112. 2.07 54.1 0 8.1.2 Tidymodels Steps in a parsnip model: Pick a model Set the engine (what library is running the model) Set the mode (if needed, e.g., regression or classification) List of models: https://tidymodels.github.io/parsnip/articles/articles/Models.html Allows you to quickly swap out and try different models library(parsnip) lm_spec &lt;- parsnip::linear_reg() %&gt;% parsnip::set_engine(engine = &quot;lm&quot;) Train/fit the model: lm_fit &lt;- parsnip::fit(lm_spec, Sale_Price ~ Gr_Liv_Area, data = ames) lm_fit ## parsnip model object ## ## Fit time: 3ms ## ## Call: ## stats::lm(formula = formula, data = data) ## ## Coefficients: ## (Intercept) Gr_Liv_Area ## 13289.6 111.7 broom::tidy(lm_fit) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 13290. 3270. 4.06 0.0000494 ## 2 Gr_Liv_Area 112. 2.07 54.1 0 "],
["predict.html", "8.2 Predict", " 8.2 Predict price_pred &lt;- stats::predict(lm_fit, new_data = ames) price_pred ## # A tibble: 2,930 x 1 ## .pred ## &lt;dbl&gt; ## 1 198255. ## 2 113367. ## 3 161731. ## 4 248964. ## 5 195239. ## 6 192447. ## 7 162736. ## 8 156258. ## 9 193787. ## 10 214786. ## # … with 2,920 more rows price_pred &lt;- price_pred %&gt;% dplyr::mutate(truth = ames$Sale_Price) price_pred ## # A tibble: 2,930 x 2 ## .pred truth ## &lt;dbl&gt; &lt;int&gt; ## 1 198255. 215000 ## 2 113367. 105000 ## 3 161731. 172000 ## 4 248964. 244000 ## 5 195239. 189900 ## 6 192447. 195500 ## 7 162736. 213500 ## 8 156258. 191500 ## 9 193787. 236500 ## 10 214786. 189000 ## # … with 2,920 more rows "],
["error-metrics.html", "8.3 Error metrics", " 8.3 Error metrics \\[ \\begin{aligned} \\text{MAE} &amp;= \\dfrac{1}{n}\\sum_{i=1}^{n} \\left| \\hat{y}_i - y_i \\right|\\\\ \\text{RMSE} &amp;= \\sqrt{\\dfrac{1}{n}\\sum_{i=1}^{n} \\left( \\hat{y}_i - y_i \\right)^2} \\end{aligned} \\] Regression Metrics Guide: https://www.h2o.ai/blog/regression-metrics-guide/ library(yardstick) ## For binary classification, the first factor level is assumed to be the event. ## Set the global option `yardstick.event_first` to `FALSE` to change this. yardstick::rmse(price_pred, truth = truth, estimate = .pred) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 56505. "],
["train-test-split.html", "8.4 Train Test Split", " 8.4 Train Test Split best way to measure model performance is to see how it performs to new data http://www.feat.engineering/ train-test-split ames_split &lt;- rsample::initial_split(ames, prop = 0.75) ames_split ## &lt;2198/732/2930&gt; dim(ames) ## [1] 2930 81 class(ames_split) ## [1] &quot;rsplit&quot; &quot;mc_split&quot; ames_train &lt;- rsample::training(ames_split) ames_test &lt;- rsample::testing(ames_split) "],
["exercise-4.html", "8.5 Exercise", " 8.5 Exercise Split the ames dataset into training and testing sets Fit a linear model Measure how the model is performing Hint: set a seed (e.g., 42) Hint: library(tidyverse), library(tidymodels) 8.5.1 Solution library(tidyverse) ## ── Attaching packages ────────────────────────────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.2.1 ✓ purrr 0.3.3 ## ✓ tibble 2.1.3 ✓ dplyr 0.8.4 ## ✓ tidyr 1.0.2 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.4.0 ## ── Conflicts ───────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() ## x readr::spec() masks yardstick::spec() library(tidymodels) ## ── Attaching packages ───────────────────────────────────────────────────────────── tidymodels 0.0.3 ── ## ✓ broom 0.5.4 ✓ recipes 0.1.9 ## ✓ dials 0.0.4 ✓ rsample 0.0.5 ## ✓ infer 0.5.1 ## ── Conflicts ──────────────────────────────────────────────────────────────── tidymodels_conflicts() ── ## x scales::discard() masks purrr::discard() ## x dplyr::filter() masks stats::filter() ## x recipes::fixed() masks stringr::fixed() ## x dplyr::lag() masks stats::lag() ## x dials::margin() masks ggplot2::margin() ## x readr::spec() masks yardstick::spec() ## x recipes::step() masks stats::step() ## x recipes::yj_trans() masks scales::yj_trans() set.seed(42) ames_split &lt;- rsample::initial_split(ames) ames_train &lt;- rsample::training(ames_split) ames_test &lt;- rsample::testing(ames_split) lm_fit &lt;- parsnip::linear_reg() %&gt;% parsnip::set_engine(engine = &quot;lm&quot;) %&gt;% parsnip::fit(Sale_Price ~ Gr_Liv_Area, data = ames_train) price_pred &lt;- lm_fit %&gt;% stats::predict(new_data = ames_test) %&gt;% dplyr::mutate(price_truth = ames_test$Sale_Price) yardstick::rmse(price_pred, truth = price_truth, estimate = .pred) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 56478. "],
["workflows.html", "Chapter 9 Workflows", " Chapter 9 Workflows Transform data: center/scale Sale_price Split data Train on the training set Evaluate/predict on the test set Data Leakage: When information from the test set “leaks” into the training data Split data Transform training set Train on training set Transformation + Evaluate/predict on test set library(AmesHousing) library(tidymodels) ## ── Attaching packages ─────────────────────────────────── tidymodels 0.0.3 ── ## ✓ broom 0.5.4 ✓ purrr 0.3.3 ## ✓ dials 0.0.4 ✓ recipes 0.1.9 ## ✓ dplyr 0.8.4 ✓ rsample 0.0.5 ## ✓ ggplot2 3.2.1 ✓ tibble 2.1.3 ## ✓ infer 0.5.1 ✓ yardstick 0.0.5 ## ✓ parsnip 0.0.5 ## ── Conflicts ────────────────────────────────────── tidymodels_conflicts() ── ## x purrr::discard() masks scales::discard() ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() ## x ggplot2::margin() masks dials::margin() ## x recipes::step() masks stats::step() ## x recipes::yj_trans() masks scales::yj_trans() lm_spec &lt;- parsnip::linear_reg() %&gt;% parsnip::set_engine(&quot;lm&quot;) ames_split &lt;- rsample::initial_split(AmesHousing::make_ames(), prop = 0.75) bb_wf &lt;- workflows::workflow() %&gt;% workflows::add_formula(Sale_Price ~ Bedroom_AbvGr + Full_Bath + Half_Bath) %&gt;% workflows::add_model(lm_spec) bb_wf ## ══ Workflow ═════════════════════════════════════════════════════════════════ ## Preprocessor: Formula ## Model: linear_reg() ## ## ── Preprocessor ───────────────────────────────────────────────────────────── ## Sale_Price ~ Bedroom_AbvGr + Full_Bath + Half_Bath ## ## ── Model ──────────────────────────────────────────────────────────────────── ## Linear Regression Model Specification (regression) ## ## Computational engine: lm # fit the final best model to the training set and evaluate the test set fit_split &lt;- tune::last_fit(bb_wf, ames_split) fit_split ## # # Monte Carlo cross-validation (0.75/0.25) with 1 resamples ## # A tibble: 1 x 6 ## splits id .metrics .notes .predictions .workflow ## * &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [2.2K… train/test … &lt;tibble [2 ×… &lt;tibble [0… &lt;tibble [732 ×… &lt;workflo… fit_split %&gt;% tune::collect_metrics() ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 58504. ## 2 rsq standard 0.354 "],
["update-formula.html", "9.1 Update formula", " 9.1 Update formula all_wf &lt;- bb_wf %&gt;% workflows::update_formula(Sale_Price ~ .) all_wf %&gt;% tune::last_fit(ames_split) %&gt;% tune::collect_metrics() ## ! Resample1: model (predictions): prediction from a rank-deficient fit may be misleading ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 38981. ## 2 rsq standard 0.758 "],
["update-model.html", "9.2 Update Model", " 9.2 Update Model Update to fit a regression decision tree rt_spec &lt;- parsnip::decision_tree() %&gt;% parsnip::set_engine(engine = &quot;rpart&quot;) %&gt;% parsnip::set_mode(&quot;regression&quot;) rt_wf &lt;- all_wf %&gt;% workflows::update_model(rt_spec) all_fitwf &lt;- rt_wf %&gt;% tune::last_fit(ames_split) all_fitwf %&gt;% tune::collect_metrics() ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 39730. ## 2 rsq standard 0.701 all_fitwf %&gt;% tune::collect_predictions() ## # A tibble: 732 x 4 ## id .pred .row Sale_Price ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 train/test split 107845. 2 105000 ## 2 train/test split 137889. 3 172000 ## 3 train/test split 167715. 8 191500 ## 4 train/test split 168788. 10 189000 ## 5 train/test split 167715. 14 171500 ## 6 train/test split 344321. 16 538000 ## 7 train/test split 194020. 17 164000 ## 8 train/test split 168788. 21 190000 ## 9 train/test split 137889. 25 149900 ## 10 train/test split 107845. 27 126000 ## # … with 722 more rows "],
["get-workflow-components.html", "9.3 Get workflow components", " 9.3 Get workflow components all_fitwf %&gt;% purrr::pluck(&quot;.workflow&quot;, 1) %&gt;% workflows::pull_workflow_fit() # return parsnip model fit ## parsnip model object ## ## Fit time: 986ms ## n= 2198 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 2198 1.487210e+13 182194.9 ## 2) Garage_Cars&lt; 2.5 1897 5.662631e+12 161558.3 ## 4) Gr_Liv_Area&lt; 1416.5 1015 1.286642e+12 134100.4 ## 8) Year_Built&lt; 1976.5 739 6.375992e+11 121546.1 ## 16) Total_Bsmt_SF&lt; 908.5 402 2.968270e+11 107845.4 * ## 17) Total_Bsmt_SF&gt;=908.5 337 1.753004e+11 137889.3 * ## 9) Year_Built&gt;=1976.5 276 2.207027e+11 167715.1 * ## 5) Gr_Liv_Area&gt;=1416.5 882 2.730102e+12 193156.7 ## 10) Exter_QualTypical&gt;=0.5 496 8.861079e+11 168788.4 * ## 11) Exter_QualTypical&lt; 0.5 386 1.170994e+12 224469.4 ## 22) Total_Bsmt_SF&lt; 1013.5 190 3.063697e+11 194020.5 * ## 23) Total_Bsmt_SF&gt;=1013.5 196 5.177049e+11 253986.2 * ## 3) Garage_Cars&gt;=2.5 301 3.310123e+12 312253.4 ## 6) Total_Bsmt_SF&lt; 1732.5 210 1.186234e+12 273081.4 ## 12) Year_Remod_Add&lt; 1990.5 24 4.891929e+10 149116.7 * ## 13) Year_Remod_Add&gt;=1990.5 186 7.209114e+11 289076.8 ## 26) Gr_Liv_Area&lt; 2323 125 2.147973e+11 262117.8 * ## 27) Gr_Liv_Area&gt;=2323 61 2.290998e+11 344320.8 * ## 7) Total_Bsmt_SF&gt;=1732.5 91 1.058039e+12 402650.4 ## 14) Gr_Liv_Area&lt; 2217 59 2.127745e+11 356679.9 * ## 15) Gr_Liv_Area&gt;=2217 32 4.906959e+11 487408.3 * all_fitwf %&gt;% purrr::pluck(&quot;.workflow&quot;, 1) %&gt;% workflows::pull_workflow_spec() ## Decision Tree Model Specification (regression) ## ## Computational engine: rpart "],
["purrr-tidymodel-objects.html", "Chapter 10 Purrr + tidymodel objects", " Chapter 10 Purrr + tidymodel objects library(tidyverse) ## ── Attaching packages ──────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.2.1 ✓ purrr 0.3.3 ## ✓ tibble 2.1.3 ✓ dplyr 0.8.4 ## ✓ tidyr 1.0.2 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.4.0 ## ── Conflicts ─────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(tidymodels) ## ── Attaching packages ─────────────────────────────────── tidymodels 0.0.3 ── ## ✓ broom 0.5.4 ✓ recipes 0.1.9 ## ✓ dials 0.0.4 ✓ rsample 0.0.5 ## ✓ infer 0.5.1 ✓ yardstick 0.0.5 ## ✓ parsnip 0.0.5 ## ── Conflicts ────────────────────────────────────── tidymodels_conflicts() ── ## x scales::discard() masks purrr::discard() ## x dplyr::filter() masks stats::filter() ## x recipes::fixed() masks stringr::fixed() ## x dplyr::lag() masks stats::lag() ## x dials::margin() masks ggplot2::margin() ## x yardstick::spec() masks readr::spec() ## x recipes::step() masks stats::step() ## x recipes::yj_trans() masks scales::yj_trans() library(AmesHousing) "],
["exercise-train-on-split-data.html", "10.1 Exercise: Train on split data", " 10.1 Exercise: Train on split data ames &lt;- AmesHousing::make_ames() ames_split &lt;- rsample::initial_split(ames, strata = Sale_Price, breaks = 4) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) lm_spec &lt;- parsnip::linear_reg() %&gt;% parsnip::set_engine(&quot;lm&quot;) lm_fit &lt;- workflows::workflow() %&gt;% workflows::add_formula(Sale_Price ~ Gr_Liv_Area) %&gt;% workflows::add_model(lm_spec) %&gt;% parsnip::fit(data = ames_train) price_pred &lt;- lm_fit %&gt;% stats::predict(new_data = ames_test) %&gt;% dplyr::mutate(price_truth = ames_test$Sale_Price) yardstick::rmse(price_pred, truth = price_truth, estimate = .pred) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 53810. "],
["pass-the-split-object.html", "10.2 Pass the split object", " 10.2 Pass the split object lm_split &lt;- workflows::workflow() %&gt;% workflows::add_formula(Sale_Price ~ Gr_Liv_Area) %&gt;% workflows::add_model(lm_spec) %&gt;% tune::last_fit(ames_split) lm_split ## # # Monte Carlo cross-validation (0.75/0.25) with 1 resamples ## # A tibble: 1 x 6 ## splits id .metrics .notes .predictions .workflow ## * &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [2.2K… train/test … &lt;tibble [2 ×… &lt;tibble [0… &lt;tibble [731 ×… &lt;workflo… "],
["question.html", "10.3 Question", " 10.3 Question What does the splits column contain simple_split &lt;- lm_split %&gt;% dplyr::select(splits, id) simple_split ## # # Monte Carlo cross-validation (0.75/0.25) with 1 resamples ## # A tibble: 1 x 2 ## splits id ## * &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [2.2K/731]&gt; train/test split "],
["exercise-5.html", "10.4 Exercise", " 10.4 Exercise How can you just return the contents of the first cell in splits? simple_split[[&quot;splits&quot;]][[1]] ## &lt;2199/731/2930&gt; simple_split[[1, 1]] ## &lt;2199/731/2930&gt; simple_split %&gt;% magrittr::extract2(&quot;splits&quot;) %&gt;% magrittr::extract2(1) ## &lt;2199/731/2930&gt; simple_split %&gt;% purrr::pluck(&quot;splits&quot;, 1) ## &lt;2199/731/2930&gt; What’s the difference between the output of these 2 results? simple_split %&gt;% pluck(&quot;splits&quot;) ## [[1]] ## &lt;2199/731/2930&gt; simple_split %&gt;% pluck(&quot;splits&quot;, 1) ## &lt;2199/731/2930&gt; "],
["map-functions.html", "10.5 map functions", " 10.5 map functions simple_split %&gt;% pluck(&quot;splits&quot;) %&gt;% map(rsample::testing) ## [[1]] ## # A tibble: 731 x 81 ## MS_SubClass MS_Zoning Lot_Frontage Lot_Area Street Alley Lot_Shape ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 One_Story_… Resident… 141 31770 Pave No_A… Slightly… ## 2 Two_Story_… Resident… 78 9978 Pave No_A… Slightly… ## 3 One_Story_… Resident… 0 7980 Pave No_A… Slightly… ## 4 One_Story_… Resident… 0 6820 Pave No_A… Slightly… ## 5 One_Story_… Resident… 85 13175 Pave No_A… Regular ## 6 One_Story_… Resident… 0 12537 Pave No_A… Slightly… ## 7 One_Story_… Resident… 70 8400 Pave No_A… Regular ## 8 One_Story_… Resident… 70 10500 Pave No_A… Regular ## 9 One_Story_… Resident… 94 12883 Pave No_A… Slightly… ## 10 One_Story_… Resident… 95 12182 Pave No_A… Regular ## # … with 721 more rows, and 74 more variables: Land_Contour &lt;fct&gt;, ## # Utilities &lt;fct&gt;, Lot_Config &lt;fct&gt;, Land_Slope &lt;fct&gt;, Neighborhood &lt;fct&gt;, ## # Condition_1 &lt;fct&gt;, Condition_2 &lt;fct&gt;, Bldg_Type &lt;fct&gt;, House_Style &lt;fct&gt;, ## # Overall_Qual &lt;fct&gt;, Overall_Cond &lt;fct&gt;, Year_Built &lt;int&gt;, ## # Year_Remod_Add &lt;int&gt;, Roof_Style &lt;fct&gt;, Roof_Matl &lt;fct&gt;, ## # Exterior_1st &lt;fct&gt;, Exterior_2nd &lt;fct&gt;, Mas_Vnr_Type &lt;fct&gt;, ## # Mas_Vnr_Area &lt;dbl&gt;, Exter_Qual &lt;fct&gt;, Exter_Cond &lt;fct&gt;, Foundation &lt;fct&gt;, ## # Bsmt_Qual &lt;fct&gt;, Bsmt_Cond &lt;fct&gt;, Bsmt_Exposure &lt;fct&gt;, ## # BsmtFin_Type_1 &lt;fct&gt;, BsmtFin_SF_1 &lt;dbl&gt;, BsmtFin_Type_2 &lt;fct&gt;, ## # BsmtFin_SF_2 &lt;dbl&gt;, Bsmt_Unf_SF &lt;dbl&gt;, Total_Bsmt_SF &lt;dbl&gt;, Heating &lt;fct&gt;, ## # Heating_QC &lt;fct&gt;, Central_Air &lt;fct&gt;, Electrical &lt;fct&gt;, First_Flr_SF &lt;int&gt;, ## # Second_Flr_SF &lt;int&gt;, Low_Qual_Fin_SF &lt;int&gt;, Gr_Liv_Area &lt;int&gt;, ## # Bsmt_Full_Bath &lt;dbl&gt;, Bsmt_Half_Bath &lt;dbl&gt;, Full_Bath &lt;int&gt;, ## # Half_Bath &lt;int&gt;, Bedroom_AbvGr &lt;int&gt;, Kitchen_AbvGr &lt;int&gt;, ## # Kitchen_Qual &lt;fct&gt;, TotRms_AbvGrd &lt;int&gt;, Functional &lt;fct&gt;, ## # Fireplaces &lt;int&gt;, Fireplace_Qu &lt;fct&gt;, Garage_Type &lt;fct&gt;, ## # Garage_Finish &lt;fct&gt;, Garage_Cars &lt;dbl&gt;, Garage_Area &lt;dbl&gt;, ## # Garage_Qual &lt;fct&gt;, Garage_Cond &lt;fct&gt;, Paved_Drive &lt;fct&gt;, ## # Wood_Deck_SF &lt;int&gt;, Open_Porch_SF &lt;int&gt;, Enclosed_Porch &lt;int&gt;, ## # Three_season_porch &lt;int&gt;, Screen_Porch &lt;int&gt;, Pool_Area &lt;int&gt;, ## # Pool_QC &lt;fct&gt;, Fence &lt;fct&gt;, Misc_Feature &lt;fct&gt;, Misc_Val &lt;int&gt;, ## # Mo_Sold &lt;int&gt;, Year_Sold &lt;int&gt;, Sale_Type &lt;fct&gt;, Sale_Condition &lt;fct&gt;, ## # Sale_Price &lt;int&gt;, Longitude &lt;dbl&gt;, Latitude &lt;dbl&gt; "],
["exercise-6.html", "10.6 Exercise", " 10.6 Exercise Create a column in simple_split that contains the training set data frame. Name the column train_set. simple_split %&gt;% dplyr::mutate(train_set = purrr::map(splits, training)) ## # # Monte Carlo cross-validation (0.75/0.25) with 1 resamples ## # A tibble: 1 x 3 ## splits id train_set ## * &lt;list&gt; &lt;chr&gt; &lt;list&gt; ## 1 &lt;split [2.2K/731]&gt; train/test split &lt;tibble [2,199 × 81]&gt; "],
["unnest.html", "10.7 unnest", " 10.7 unnest Expanding a list column simple_split %&gt;% dplyr::mutate(train_set = purrr::map(splits, training)) %&gt;% tidyr::unnest(train_set) ## # A tibble: 2,199 x 83 ## splits id MS_SubClass MS_Zoning Lot_Frontage Lot_Area Street Alley ## &lt;list&gt; &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; ## 1 &lt;spli… trai… One_Story_… Resident… 80 11622 Pave No_A… ## 2 &lt;spli… trai… One_Story_… Resident… 81 14267 Pave No_A… ## 3 &lt;spli… trai… One_Story_… Resident… 93 11160 Pave No_A… ## 4 &lt;spli… trai… Two_Story_… Resident… 74 13830 Pave No_A… ## 5 &lt;spli… trai… One_Story_… Resident… 41 4920 Pave No_A… ## 6 &lt;spli… trai… One_Story_… Resident… 43 5005 Pave No_A… ## 7 &lt;spli… trai… One_Story_… Resident… 39 5389 Pave No_A… ## 8 &lt;spli… trai… Two_Story_… Resident… 60 7500 Pave No_A… ## 9 &lt;spli… trai… Two_Story_… Resident… 75 10000 Pave No_A… ## 10 &lt;spli… trai… Two_Story_… Resident… 63 8402 Pave No_A… ## # … with 2,189 more rows, and 75 more variables: Lot_Shape &lt;fct&gt;, ## # Land_Contour &lt;fct&gt;, Utilities &lt;fct&gt;, Lot_Config &lt;fct&gt;, Land_Slope &lt;fct&gt;, ## # Neighborhood &lt;fct&gt;, Condition_1 &lt;fct&gt;, Condition_2 &lt;fct&gt;, Bldg_Type &lt;fct&gt;, ## # House_Style &lt;fct&gt;, Overall_Qual &lt;fct&gt;, Overall_Cond &lt;fct&gt;, ## # Year_Built &lt;int&gt;, Year_Remod_Add &lt;int&gt;, Roof_Style &lt;fct&gt;, Roof_Matl &lt;fct&gt;, ## # Exterior_1st &lt;fct&gt;, Exterior_2nd &lt;fct&gt;, Mas_Vnr_Type &lt;fct&gt;, ## # Mas_Vnr_Area &lt;dbl&gt;, Exter_Qual &lt;fct&gt;, Exter_Cond &lt;fct&gt;, Foundation &lt;fct&gt;, ## # Bsmt_Qual &lt;fct&gt;, Bsmt_Cond &lt;fct&gt;, Bsmt_Exposure &lt;fct&gt;, ## # BsmtFin_Type_1 &lt;fct&gt;, BsmtFin_SF_1 &lt;dbl&gt;, BsmtFin_Type_2 &lt;fct&gt;, ## # BsmtFin_SF_2 &lt;dbl&gt;, Bsmt_Unf_SF &lt;dbl&gt;, Total_Bsmt_SF &lt;dbl&gt;, Heating &lt;fct&gt;, ## # Heating_QC &lt;fct&gt;, Central_Air &lt;fct&gt;, Electrical &lt;fct&gt;, First_Flr_SF &lt;int&gt;, ## # Second_Flr_SF &lt;int&gt;, Low_Qual_Fin_SF &lt;int&gt;, Gr_Liv_Area &lt;int&gt;, ## # Bsmt_Full_Bath &lt;dbl&gt;, Bsmt_Half_Bath &lt;dbl&gt;, Full_Bath &lt;int&gt;, ## # Half_Bath &lt;int&gt;, Bedroom_AbvGr &lt;int&gt;, Kitchen_AbvGr &lt;int&gt;, ## # Kitchen_Qual &lt;fct&gt;, TotRms_AbvGrd &lt;int&gt;, Functional &lt;fct&gt;, ## # Fireplaces &lt;int&gt;, Fireplace_Qu &lt;fct&gt;, Garage_Type &lt;fct&gt;, ## # Garage_Finish &lt;fct&gt;, Garage_Cars &lt;dbl&gt;, Garage_Area &lt;dbl&gt;, ## # Garage_Qual &lt;fct&gt;, Garage_Cond &lt;fct&gt;, Paved_Drive &lt;fct&gt;, ## # Wood_Deck_SF &lt;int&gt;, Open_Porch_SF &lt;int&gt;, Enclosed_Porch &lt;int&gt;, ## # Three_season_porch &lt;int&gt;, Screen_Porch &lt;int&gt;, Pool_Area &lt;int&gt;, ## # Pool_QC &lt;fct&gt;, Fence &lt;fct&gt;, Misc_Feature &lt;fct&gt;, Misc_Val &lt;int&gt;, ## # Mo_Sold &lt;int&gt;, Year_Sold &lt;int&gt;, Sale_Type &lt;fct&gt;, Sale_Condition &lt;fct&gt;, ## # Sale_Price &lt;int&gt;, Longitude &lt;dbl&gt;, Latitude &lt;dbl&gt; lm_split ## # # Monte Carlo cross-validation (0.75/0.25) with 1 resamples ## # A tibble: 1 x 6 ## splits id .metrics .notes .predictions .workflow ## * &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [2.2K… train/test … &lt;tibble [2 ×… &lt;tibble [0… &lt;tibble [731 ×… &lt;workflo… lm_split %&gt;% tidyr::unnest(.metrics) ## # A tibble: 2 x 8 ## splits id .metric .estimator .estimate .notes .predictions .workflow ## &lt;list&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [… train/… rmse standard 53810. &lt;tibbl… &lt;tibble [731… &lt;workflo… ## 2 &lt;split [… train/… rsq standard 0.529 &lt;tibbl… &lt;tibble [731… &lt;workflo… collect_metrics is a shortcut. lm_split %&gt;% tune::collect_metrics() ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 53810. ## 2 rsq standard 0.529 "],
["other-models.html", "Chapter 11 Other models ", " Chapter 11 Other models "],
["exercise-7.html", "11.1 Exercise", " 11.1 Exercise Load the ames dataset Split the data, rsplit Fit a linear model and test it define model spec create workflow fit model calculate the rmse 11.1.1 Solution library(tidyverse) ## ── Attaching packages ──────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.2.1 ✓ purrr 0.3.3 ## ✓ tibble 2.1.3 ✓ dplyr 0.8.4 ## ✓ tidyr 1.0.2 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.4.0 ## ── Conflicts ─────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(tidymodels) ## ── Attaching packages ─────────────────────────────────── tidymodels 0.0.3 ── ## ✓ broom 0.5.4 ✓ recipes 0.1.9 ## ✓ dials 0.0.4 ✓ rsample 0.0.5 ## ✓ infer 0.5.1 ✓ yardstick 0.0.5 ## ✓ parsnip 0.0.5 ## ── Conflicts ────────────────────────────────────── tidymodels_conflicts() ── ## x scales::discard() masks purrr::discard() ## x dplyr::filter() masks stats::filter() ## x recipes::fixed() masks stringr::fixed() ## x dplyr::lag() masks stats::lag() ## x dials::margin() masks ggplot2::margin() ## x yardstick::spec() masks readr::spec() ## x recipes::step() masks stats::step() ## x recipes::yj_trans() masks scales::yj_trans() library(AmesHousing) ames &lt;- AmesHousing::make_ames() ames_split &lt;- rsample::initial_split(ames) lm_spec &lt;- parsnip::linear_reg() %&gt;% parsnip::set_engine(&quot;lm&quot;) lm_wf &lt;- workflows::workflow() %&gt;% workflows::add_formula(Sale_Price ~ Gr_Liv_Area) %&gt;% workflows::add_model(lm_spec) lm_split &lt;- lm_wf %&gt;% tune::last_fit(ames_split) lm_split %&gt;% tune::collect_metrics() ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 56478. ## 2 rsq standard 0.435 "],
["decision-trees.html", "11.2 Decision trees", " 11.2 Decision trees https://tidymodels.github.io/parsnip/articles/articles/Models.html Fit a decision tree using rpart rt_spec &lt;- parsnip::decision_tree() %&gt;% parsnip::set_engine(&quot;rpart&quot;) %&gt;% parsnip::set_mode(&quot;regression&quot;) rt_wf &lt;- lm_wf %&gt;% workflows::update_model(rt_spec) rt_wf ## ══ Workflow ═════════════════════════════════════════════════════════════════ ## Preprocessor: Formula ## Model: decision_tree() ## ## ── Preprocessor ───────────────────────────────────────────────────────────── ## Sale_Price ~ Gr_Liv_Area ## ## ── Model ──────────────────────────────────────────────────────────────────── ## Decision Tree Model Specification (regression) ## ## Computational engine: rpart rt_wf %&gt;% tune::last_fit(ames_split) %&gt;% tune::collect_metrics() ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 54906. ## 2 rsq standard 0.452 "],
["knn.html", "11.3 KNN", " 11.3 KNN knn_spec &lt;- parsnip::nearest_neighbor() %&gt;% parsnip::set_engine(&quot;kknn&quot;) %&gt;% parsnip::set_mode(&quot;regression&quot;) knn_wf &lt;- rt_wf %&gt;% workflows::update_model(knn_spec) knn_wf %&gt;% tune::last_fit(ames_split) %&gt;% tune::collect_metrics() ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 58943. ## 2 rsq standard 0.418 lm_wf %&gt;% tune::last_fit(ames_split) %&gt;% tune::collect_metrics() ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 56478. ## 2 rsq standard 0.435 rt_wf %&gt;% tune::last_fit(ames_split) %&gt;% tune::collect_metrics() ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 54906. ## 2 rsq standard 0.452 "],
["selecting-your-metrics.html", "11.4 Selecting your metrics", " 11.4 Selecting your metrics https://tidymodels.github.io/yardstick/reference/index.html rt_wf %&gt;% tune::last_fit( ames_split, metrics = yardstick::metric_set( yardstick::rmse, yardstick::rsq, yardstick::mae )) %&gt;% tune::collect_metrics() ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 54906. ## 2 rsq standard 0.452 ## 3 mae standard 38035. "],
["recipes.html", "Chapter 12 Recipes", " Chapter 12 Recipes The pre-processing step https://tidymodels.github.io/recipes/reference/index.html library(tidymodels) ## ── Attaching packages ─────────────────────────────────── tidymodels 0.0.3 ── ## ✓ broom 0.5.4 ✓ purrr 0.3.3 ## ✓ dials 0.0.4 ✓ recipes 0.1.9 ## ✓ dplyr 0.8.4 ✓ rsample 0.0.5 ## ✓ ggplot2 3.2.1 ✓ tibble 2.1.3 ## ✓ infer 0.5.1 ✓ yardstick 0.0.5 ## ✓ parsnip 0.0.5 ## ── Conflicts ────────────────────────────────────── tidymodels_conflicts() ── ## x purrr::discard() masks scales::discard() ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() ## x ggplot2::margin() masks dials::margin() ## x recipes::step() masks stats::step() ## x recipes::yj_trans() masks scales::yj_trans() library(AmesHousing) ames &lt;- AmesHousing::make_ames() rec &lt;- recipes::recipe(Sale_Price ~ ., data = ames) rec %&gt;% step_novel(all_nominal()) %&gt;% step_zv(all_predictors()) ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 80 ## ## Operations: ## ## Novel factor level assignment for all_nominal ## Zero variance filter on all_predictors "],
["pca.html", "12.1 PCA", " 12.1 PCA rec &lt;- recipes::recipe(Sale_Price ~ ., data = ames) %&gt;% recipes::step_center(all_numeric()) %&gt;% recipes::step_scale(all_numeric()) 12.1.1 Prep/bake prep: trains a dataset bake: apply a trained model to new data However, you do not need to do this since the fit functions do this for you too rec %&gt;% recipes::prep(training = ames) %&gt;% recipes::bake(new_data = ames) ## # A tibble: 2,930 x 81 ## MS_SubClass MS_Zoning Lot_Frontage Lot_Area Street Alley Lot_Shape ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 One_Story_… Resident… 2.49 2.74 Pave No_A… Slightly… ## 2 One_Story_… Resident… 0.667 0.187 Pave No_A… Regular ## 3 One_Story_… Resident… 0.697 0.523 Pave No_A… Slightly… ## 4 One_Story_… Resident… 1.06 0.128 Pave No_A… Regular ## 5 Two_Story_… Resident… 0.488 0.467 Pave No_A… Slightly… ## 6 Two_Story_… Resident… 0.608 -0.0216 Pave No_A… Slightly… ## 7 One_Story_… Resident… -0.497 -0.663 Pave No_A… Regular ## 8 One_Story_… Resident… -0.437 -0.653 Pave No_A… Slightly… ## 9 One_Story_… Resident… -0.557 -0.604 Pave No_A… Slightly… ## 10 Two_Story_… Resident… 0.0702 -0.336 Pave No_A… Regular ## # … with 2,920 more rows, and 74 more variables: Land_Contour &lt;fct&gt;, ## # Utilities &lt;fct&gt;, Lot_Config &lt;fct&gt;, Land_Slope &lt;fct&gt;, Neighborhood &lt;fct&gt;, ## # Condition_1 &lt;fct&gt;, Condition_2 &lt;fct&gt;, Bldg_Type &lt;fct&gt;, House_Style &lt;fct&gt;, ## # Overall_Qual &lt;fct&gt;, Overall_Cond &lt;fct&gt;, Year_Built &lt;dbl&gt;, ## # Year_Remod_Add &lt;dbl&gt;, Roof_Style &lt;fct&gt;, Roof_Matl &lt;fct&gt;, ## # Exterior_1st &lt;fct&gt;, Exterior_2nd &lt;fct&gt;, Mas_Vnr_Type &lt;fct&gt;, ## # Mas_Vnr_Area &lt;dbl&gt;, Exter_Qual &lt;fct&gt;, Exter_Cond &lt;fct&gt;, Foundation &lt;fct&gt;, ## # Bsmt_Qual &lt;fct&gt;, Bsmt_Cond &lt;fct&gt;, Bsmt_Exposure &lt;fct&gt;, ## # BsmtFin_Type_1 &lt;fct&gt;, BsmtFin_SF_1 &lt;dbl&gt;, BsmtFin_Type_2 &lt;fct&gt;, ## # BsmtFin_SF_2 &lt;dbl&gt;, Bsmt_Unf_SF &lt;dbl&gt;, Total_Bsmt_SF &lt;dbl&gt;, Heating &lt;fct&gt;, ## # Heating_QC &lt;fct&gt;, Central_Air &lt;fct&gt;, Electrical &lt;fct&gt;, First_Flr_SF &lt;dbl&gt;, ## # Second_Flr_SF &lt;dbl&gt;, Low_Qual_Fin_SF &lt;dbl&gt;, Gr_Liv_Area &lt;dbl&gt;, ## # Bsmt_Full_Bath &lt;dbl&gt;, Bsmt_Half_Bath &lt;dbl&gt;, Full_Bath &lt;dbl&gt;, ## # Half_Bath &lt;dbl&gt;, Bedroom_AbvGr &lt;dbl&gt;, Kitchen_AbvGr &lt;dbl&gt;, ## # Kitchen_Qual &lt;fct&gt;, TotRms_AbvGrd &lt;dbl&gt;, Functional &lt;fct&gt;, ## # Fireplaces &lt;dbl&gt;, Fireplace_Qu &lt;fct&gt;, Garage_Type &lt;fct&gt;, ## # Garage_Finish &lt;fct&gt;, Garage_Cars &lt;dbl&gt;, Garage_Area &lt;dbl&gt;, ## # Garage_Qual &lt;fct&gt;, Garage_Cond &lt;fct&gt;, Paved_Drive &lt;fct&gt;, ## # Wood_Deck_SF &lt;dbl&gt;, Open_Porch_SF &lt;dbl&gt;, Enclosed_Porch &lt;dbl&gt;, ## # Three_season_porch &lt;dbl&gt;, Screen_Porch &lt;dbl&gt;, Pool_Area &lt;dbl&gt;, ## # Pool_QC &lt;fct&gt;, Fence &lt;fct&gt;, Misc_Feature &lt;fct&gt;, Misc_Val &lt;dbl&gt;, ## # Mo_Sold &lt;dbl&gt;, Year_Sold &lt;dbl&gt;, Sale_Type &lt;fct&gt;, Sale_Condition &lt;fct&gt;, ## # Longitude &lt;dbl&gt;, Latitude &lt;dbl&gt;, Sale_Price &lt;dbl&gt; 12.1.2 Dummy variables aka one-hot encoding You don’t need this for decision trees or ensembles of trees rec %&gt;% recipes::step_dummy(all_nominal()) ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 80 ## ## Operations: ## ## Centering for all_numeric ## Scaling for all_numeric ## Dummy variables from all_nominal 12.1.3 Step novel A catch all for new categories that the model may not have trained on Do this before dummy encoding rec %&gt;% recipes::step_novel(all_nominal()) %&gt;% recipes::step_dummy(all_nominal()) ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 80 ## ## Operations: ## ## Centering for all_numeric ## Scaling for all_numeric ## Novel factor level assignment for all_nominal ## Dummy variables from all_nominal 12.1.4 remove 0 variance Remove columns where there is only 1 value in it rec %&gt;% recipes::step_novel(all_nominal()) %&gt;% recipes::step_dummy(all_nominal()) %&gt;% recipes::step_zv(all_predictors()) ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 80 ## ## Operations: ## ## Centering for all_numeric ## Scaling for all_numeric ## Novel factor level assignment for all_nominal ## Dummy variables from all_nominal ## Zero variance filter on all_predictors 12.1.5 PCA rec %&gt;% recipes::step_novel(all_nominal()) %&gt;% recipes::step_dummy(all_nominal()) %&gt;% recipes::step_zv(all_predictors()) %&gt;% recipes::step_pca(all_numeric(), num_comp = 5) ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 80 ## ## Operations: ## ## Centering for all_numeric ## Scaling for all_numeric ## Novel factor level assignment for all_nominal ## Dummy variables from all_nominal ## Zero variance filter on all_predictors ## No PCA components were extracted. "],
["exercise-8.html", "12.2 Exercise", " 12.2 Exercise Write a recipe for the Sale_Price ~ . variables that: Adds a novel level to all factors Converts all factors to dummy variables Catches any zero variance variables Centers all of the predictors Scales all of the predictors Computes the first 5 principal components Save the result as pca_rec 12.2.1 Solution pca_rec &lt;- recipe(Sale_Price ~ ., data = ames) %&gt;% step_novel(all_nominal()) %&gt;% step_dummy(all_nominal()) %&gt;% step_zv(all_predictors()) %&gt;% step_center(all_predictors()) %&gt;% step_scale(all_predictors()) %&gt;% step_pca(all_predictors(), num_comp = 5) "],
["put-it-all-together.html", "12.3 Put it all together", " 12.3 Put it all together Split the ames data (rsample) Create a lm_spec (parsnip) Create a pca_rec recipe (recipe) Create a workflow (workflows) Fit model (tune) 12.3.1 Solution ames_split &lt;- rsample::initial_split(ames) ames_train &lt;- rsample::training(ames_split) ames_test &lt;- rsample::testing(ames_split) lm_spec &lt;- parsnip::linear_reg() %&gt;% parsnip::set_engine(&quot;lm&quot;) pca_rec &lt;- recipe(Sale_Price ~ ., data = ames_train) %&gt;% step_novel(all_nominal()) %&gt;% step_dummy(all_nominal()) %&gt;% step_zv(all_predictors()) %&gt;% step_center(all_predictors()) %&gt;% step_scale(all_predictors()) %&gt;% step_pca(all_predictors(), num_comp = 5) pca_wf &lt;- workflows::workflow() %&gt;% workflows::add_recipe(pca_rec) %&gt;% workflows::add_model(lm_spec) pca_wf ## ══ Workflow ═════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: linear_reg() ## ## ── Preprocessor ───────────────────────────────────────────────────────────── ## 6 Recipe Steps ## ## ● step_novel() ## ● step_dummy() ## ● step_zv() ## ● step_center() ## ● step_scale() ## ● step_pca() ## ## ── Model ──────────────────────────────────────────────────────────────────── ## Linear Regression Model Specification (regression) ## ## Computational engine: lm pca_wf %&gt;% tune::last_fit(ames_split) %&gt;% tune::collect_metrics() ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 40883. ## 2 rsq standard 0.710 "],
["rmarkdown.html", "Chapter 13 RMarkdown", " Chapter 13 RMarkdown https://resources.rstudio.com/rstudio-conf-2020/one-r-markdown-document-fourteen-demosyihui-xie https://yihui.org/en/2020/02/rstudio-conf-2020/ "],
["html.html", "13.1 html", " 13.1 html html_document: number_sections: true theme: journal toc: true "],
["pdf.html", "13.2 pdf", " 13.2 pdf latex_document: extra_dependencies: [animate, coffee4] "],
["word.html", "13.3 word", " 13.3 word word_document: reference_docx: rstudio-conf20.docx "],
["powerpoint.html", "13.4 powerpoint", " 13.4 powerpoint powerpoint_presentation: reference_doc: rstudio-conf20.pptx "],
["ioslides.html", "13.5 ioslides", " 13.5 ioslides # o for overview mode; w for widescreen ioslides_presentation: default "],
["beamer.html", "13.6 beamer", " 13.6 beamer beamer_presentation: slide_level: 2 theme: AnnArbor "],
["tufte.html", "13.7 tufte", " 13.7 tufte tufte::tufte_html: tufte_variant: envisioned "],
["rolldown.html", "13.8 rolldown", " 13.8 rolldown rolldown::scrollama_sidebar: default "],
["rticles.html", "13.9 rticles", " 13.9 rticles rticles::jss_article: pandoc_args: [-V, documentclass=jss] extra_dependencies: animate "],
["flexdashboard.html", "13.10 flexdashboard", " 13.10 flexdashboard flexdashboard::flex_dashboard: default "],
["pagedown.html", "13.11 pagedown", " 13.11 pagedown pagedown::book_crc: default "],
["bookdown.html", "13.12 bookdown", " 13.12 bookdown bookdown::gitbook: default "],
["learnr.html", "13.13 learnr", " 13.13 learnr learnr::tutorial: default runtime: shiny_prerendered "],
["bookdown-1.html", "Chapter 14 Bookdown", " Chapter 14 Bookdown https://bookdown.org/yihui/bookdown/ install.packages(&quot;bookdown&quot;) Minimal bookdown example: Code: https://github.com/rstudio/bookdown-demo Site: https://bookdown.org/yihui/bookdown-demo/ "],
["components-of-a-book.html", "14.1 Components of a book", " 14.1 Components of a book 14.1.1 Section titles: # Preface {-} # Part 1: The beginning {-} # Chapter 1: Introduction ## Example One ## Example Two "],
["rendering-m-k-vs-k-m.html", "14.2 Rendering M-K vs K-M", " 14.2 Rendering M-K vs K-M M-K: merge all documents first, then knit K-M: Knit each document separately, then merge together Default is M-K, but you can change this option in _bookdown.yml, new_session: yes vs new_session: no "],
["looking-at-the-book-while-you-work.html", "14.3 Looking at the book while you work", " 14.3 Looking at the book while you work Preview book add-on Will recompile the book on each save "],
["figures.html", "14.4 Figures", " 14.4 Figures library(ggplot2) ggplot(diamonds, aes(x = carat, y = price)) + geom_point() ggplot(diamonds, aes(x = carat, y = price, color = color)) + geom_point() Figure 14.1: A figure caption. ` ``{r diamond-colored, fig.cap = &#39;A figure caption.&#39;} ggplot(diamonds, aes(x = carat, y = price, color = color)) + geom_point() ` `` You can reference Figure 14.1 by the chunk name with \\@ref(fig:diamond-colored) "],
["tables.html", "14.5 Tables", " 14.5 Tables knitr::kable( head(mtcars[, 1:8], 10), booktabs = TRUE, caption = &#39;A table of the first 10 rows of the mtcars data.&#39; ) Table 14.1: A table of the first 10 rows of the mtcars data. mpg cyl disp hp drat wt qsec vs Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 ` ``{r, mtcars-head-table} knitr::kable( head(mtcars[, 1:8], 10), booktabs = TRUE, caption = &#39;A table of the first 10 rows of the mtcars data.&#39; ) ` `` Table 14.1 can be referenced with \\@ref(tab:mtcars-head-table) "],
["cross-ref.html", "14.6 Cross refrence chapters", " 14.6 Cross refrence chapters Cross references to other chapters can be done with \\@ref(label). Reference labels are auto created for you, but it’s better to create your own labels with {#id}. E.g., this chapter, Section 14.6, was done by placing ## Cross refrence chapters {#cross-ref} as the header. "],
["references.html", "References", " References Ahmadia, Aron, James Allen, Alison Appling, Sean Aubin, Pete Bachant, Piotr Banaszkiewicz, Pauline Barmby, et al. 2016. “Software Carpentry: Version Control with Git.” https://doi.org/10.5281/zenodo.57467. Hannay, Jo Erskine, Hans Petter Langtangen, Carolyn MacLeod, Dietmar Pfahl, Janice Singer, and Greg Wilson. 2009. “How Do Scientists Develop and Use Scientific Software?” In Proc. 2009 Icse Workshop on Software Engineering for Computational Science and Engineering. Wilson, Greg. 2006. “Software Carpentry: Getting Scientists to Write Better Code by Making Them More Productive.” Computing in Science &amp; Engineering. ———. 2008. “Those Who Will Not Learn from History...” Computing in Science &amp; Engineering. ———. 2016. “Software Carpentry: Lessons Learned.” January 28, 2016. https://doi.org/10.12688/f1000research.3-62.v2. ———. n.d. “Software Carpentry Web Site.” http://software-carpentry.org. Wilson, Gregory. 2009. “How Do Scientists Really Use Computers?” American Scientist. Wilson, Gregory V. 2005. “Where’s the Real Bottleneck in Scientific Computing?” American Scientist. "]
]
